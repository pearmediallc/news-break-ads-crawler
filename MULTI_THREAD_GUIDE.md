# ğŸš€ Multi-Threading Ad Extraction Guide

## Overview

Version 1.5.0 introduces **multi-threaded extraction** with performance optimizations that dramatically increase ad discovery rates.

### **Performance Improvements**

| Configuration | Expected Performance | Previous Performance |
|--------------|---------------------|---------------------|
| **Single Thread** | 500-2000 ads/day | ~50 ads/3 days |
| **3 Workers** | 1500-6000 ads/day | N/A |
| **5 Workers (Default)** | 2500-10,000 ads/day | N/A |
| **10 Workers** | 5000-20,000 ads/day | N/A |

---

## ğŸ”§ Key Optimizations Applied

### 1. **Sliding Window Deduplication**
- Keeps only last 500 ad signatures in memory
- Allows re-extraction of ads after cache rotation
- Prevents permanent blocking of recurring ads

### 2. **Faster URL Rotation**
- Rotates to new URL after 12 failed attempts (was 50)
- Reduces time stuck on stale content from 2.5 min â†’ 36 sec

### 3. **Faster Page Refresh**
- Refreshes page after 8 failed attempts (was 20)
- Loads new ad inventory faster

### 4. **Improved Scroll Speed**
- 1.5 second scroll intervals (was 2.5 seconds)
- 40% faster content discovery

### 5. **Periodic Cache Cleanup**
- Automatic cleanup every 10 minutes
- Ensures cache stays within limits

---

## ğŸ¯ Quick Start

### **Option 1: Command Line (Recommended)**

```bash
# Start with 5 workers (default)
npm run multi-thread

# Start with 3 workers
npm run multi-thread:3

# Start with 5 workers
npm run multi-thread:5

# Start with 10 workers
npm run multi-thread:10

# Custom configuration
node start-multi-thread.js --workers 7 --url "https://www.newsbreak.com/chicago-il"
```

### **Option 2: API Endpoint**

The `/api/extract/start` endpoint now uses multi-threading by default for unlimited mode:

```javascript
POST /api/extract/start
{
  "url": "https://www.newsbreak.com/new-york-ny",
  "extractionMode": "unlimited",
  "deviceMode": "desktop",
  "useMultiThread": true,  // Default: true
  "maxWorkers": 5          // Default: 5
}
```

### **Option 3: Direct API (Advanced)**

```javascript
POST /api/extract/multi-thread/start
{
  "maxWorkers": 5,
  "deviceMode": "desktop",
  "sameUrl": false,  // false = different cities, true = same URL
  "url": "https://www.newsbreak.com/new-york-ny"
}
```

---

## ğŸ“Š Multi-Threading Modes

### **Mode 1: Different URLs (Recommended)**
```bash
node start-multi-thread.js --workers 5
```
- Each worker scrapes a different city
- Auto-rotates through: New York, LA, Chicago, Houston, Phoenix, etc.
- **Best for:** Maximum ad diversity
- **Performance:** Highest unique ad count

### **Mode 2: Same URL**
```bash
node start-multi-thread.js --workers 5 --same-url
```
- All workers scrape the same city
- Each worker has independent browser + scroll position
- **Best for:** Deep extraction of single market
- **Performance:** Higher total volume, some duplicates

---

## ğŸ® CLI Commands

```bash
# Help
node start-multi-thread.js --help

# Examples
node start-multi-thread.js                    # 5 workers, different URLs
node start-multi-thread.js -w 3               # 3 workers
node start-multi-thread.js --same-url         # All on same URL
node start-multi-thread.js -w 10 --mobile     # 10 workers, mobile mode
node start-multi-thread.js --url "https://www.newsbreak.com/chicago-il" -w 7
```

---

## ğŸ“¡ API Endpoints

### **Start Multi-Thread Extraction**
```bash
POST /api/extract/multi-thread/start
```

**Request Body:**
```json
{
  "maxWorkers": 5,
  "deviceMode": "desktop",
  "sameUrl": false,
  "url": "https://www.newsbreak.com/new-york-ny"
}
```

### **Stop Multi-Thread Extraction**
```bash
POST /api/extract/multi-thread/stop
```

### **Get Status**
```bash
GET /api/extract/multi-thread/status
```

**Response:**
```json
{
  "success": true,
  "isRunning": true,
  "sessionId": "multi_1234567890",
  "runtime": 600000,
  "maxWorkers": 5,
  "activeWorkers": 5,
  "totalAds": 1247,
  "workers": [
    {
      "workerId": 1,
      "url": "https://www.newsbreak.com/new-york-ny",
      "status": "running",
      "adsExtracted": 287,
      "uptime": 600000
    }
  ]
}
```

### **Get Worker Logs**
```bash
GET /api/extract/multi-thread/logs?workerId=1&limit=50
```

---

## ğŸ” Monitoring

### **Real-time Stats**
Multi-thread extractor reports stats every 30 seconds:

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“Š MULTI-THREAD EXTRACTION STATS
â±ï¸  Runtime: 30 minutes
ğŸ‘· Active Workers: 5/5
ğŸ“¦ Total Ads Extracted: 1247
âš¡ Rate: 41.6 ads/min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Worker #1: 287 ads, 30m uptime, running
  Worker #2: 265 ads, 30m uptime, running
  Worker #3: 231 ads, 30m uptime, running
  Worker #4: 248 ads, 30m uptime, running
  Worker #5: 216 ads, 30m uptime, running
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### **Worker Health Monitoring**
- Auto-restart on failure
- URL rotation for stuck workers
- Memory usage monitoring
- Error tracking per worker

---

## ğŸ’¾ Data Storage

All workers save to the **same shared database**:
- **SQLite:** `data/ads_crawler.db`
- **Session Files:** `data/sessions/worker_*.json`
- Automatic deduplication across all workers
- Real-time database writes

---

## âš™ï¸ Configuration

### **Recommended Settings**

| Environment | Workers | Mode | Expected Rate |
|------------|---------|------|---------------|
| **Development (Local)** | 3 | Different URLs | ~100 ads/hour |
| **Production (512MB RAM)** | 3-5 | Different URLs | ~150-250 ads/hour |
| **Production (1GB+ RAM)** | 5-10 | Different URLs | ~250-500 ads/hour |

### **Memory Considerations**

Each worker runs a separate browser instance:
- **1 worker** â‰ˆ 100-150 MB RAM
- **3 workers** â‰ˆ 300-450 MB RAM
- **5 workers** â‰ˆ 500-750 MB RAM
- **10 workers** â‰ˆ 1-1.5 GB RAM

---

## ğŸ›‘ Stopping Extraction

### **CLI:**
```bash
# Press Ctrl+C in terminal
^C
ğŸ›‘ Received SIGINT, stopping extraction...
```

### **API:**
```bash
POST /api/extract/multi-thread/stop
```

### **Graceful Shutdown:**
- Saves all current progress
- Closes browser instances
- Writes final stats
- Can resume later

---

## ğŸ“ˆ Performance Tips

1. **Start with 5 workers** - Good balance of speed and resource usage
2. **Use "Different URLs" mode** - More unique ads discovered
3. **Monitor memory usage** - Scale workers based on available RAM
4. **Let it run overnight** - Continuous extraction maximizes results
5. **Check logs regularly** - Identify and fix any stuck workers

---

## ğŸ› Troubleshooting

### **Workers Keep Restarting**
- Check available memory: `node start-multi-thread.js -w 3` (reduce workers)
- Check Puppeteer installation: `npm install`

### **Low Ad Count**
- Verify URL rotation is working (check logs)
- Ensure different URLs mode is enabled
- Check database for duplicates: `sqlite3 data/ads_crawler.db "SELECT COUNT(*) FROM ads;"`

### **High Memory Usage**
- Reduce number of workers
- Enable aggressive memory management (already enabled)
- Restart periodically (automatic)

---

## ğŸ“ Version History

**v1.5.0** - Multi-Threading + Performance Fixes
- Multi-threaded extraction (up to 10 workers)
- Sliding window deduplication (500 ad cache)
- Faster URL rotation (12 attempts)
- Faster page refresh (8 attempts)
- Improved scroll speed (1.5s intervals)
- Periodic cache cleanup (10 min)
- Expected: 10-40x performance increase

**v1.4.0** - Previous Version
- Single-threaded extraction
- Unlimited deduplication cache
- Slow URL rotation (50 attempts)
- Limited to ~50 ads per 3 days

---

## ğŸ‰ Expected Results

With the new multi-threading + optimizations:

```
Day 1:   2,000 - 5,000 ads
Day 2:   4,000 - 10,000 ads (cumulative)
Day 3:   6,000 - 15,000 ads (cumulative)
Week 1:  15,000 - 50,000 ads
```

Compare to previous version:
```
Day 3:   50 ads âŒ
```

---

## ğŸ†˜ Support

For issues or questions:
1. Check logs in terminal or `/api/extract/multi-thread/logs`
2. Verify worker status: `/api/extract/multi-thread/status`
3. Review database: `data/ads_crawler.db`
4. Check system resources (memory, CPU)

---

**Happy Extracting! ğŸš€**
